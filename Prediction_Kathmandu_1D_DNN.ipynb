{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction_Kathmandu_1D_DNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tan12uja/Deep_Learning_Urban_Area_Mapping/blob/master/Prediction_Kathmandu_1D_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIfBsgi8dNK"
      },
      "source": [
        "#@title Copyright 2020 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8adBmw-5m3"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an Earth Engine <> TensorFlow demonstration notebook.  Specifically, this notebook shows:\n",
        "\n",
        "1.   Making predictions on image data exported from Earth Engine in TFRecord format, here used is Kathmandu Valley.\n",
        "2.   Ingesting classified image data to Earth Engine in TFRecord format.\n",
        "\n",
        "This is intended to demonstrate a complete i/o pipeline.  For a worflow that uses a [Google AI Platform](https://cloud.google.com/ai-platform) hosted model making predictions interactively, see [this example notebook](http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/Earth_Engine_TensorFlow_AI_Platform.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiTyR3FNlv-O"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Import software libraries and/or authenticate as necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEM3FP4YakJg"
      },
      "source": [
        "## Authenticate to Colab and Cloud\n",
        "\n",
        "To read/write from a Google Cloud Storage bucket to which you have access, it's necessary to authenticate (as yourself).  *This should be the same account you use to login to Earth Engine*.  When you run the code below, it will display a link in the output to an authentication page in your browser.  Follow the link to a page that will let you grant permission to the Cloud SDK to access your resources.  Copy the code from the permissions page back into this notebook and press return to complete the process.\n",
        "\n",
        "(You may need to run this again if you get a credentials error later.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYyTIPLsvMWl",
        "cellView": "code"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejxa1MQjEGv9"
      },
      "source": [
        "## Authenticate to Earth Engine\n",
        "\n",
        "Authenticate to Earth Engine the same way you did to the Colab notebook.  Specifically, run the code to display a link to a permissions page.  This gives you access to your Earth Engine account.  *This should be the same account you used to login to Cloud previously*.  Copy the code from the Earth Engine permissions page back into the notebook and press return to complete the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzwiVqbcmJIX",
        "cellView": "code"
      },
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ70EsoWND_0"
      },
      "source": [
        "## Test the TensorFlow installation\n",
        "\n",
        "Import the TensorFlow library and check the version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1PrYRLaVw_g",
        "cellView": "code"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Xcvjp6cLOL"
      },
      "source": [
        "## Test the Folium installation\n",
        "\n",
        "We will use the Folium library for visualization.  Import the library and check the version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiVgOXzBZJSn"
      },
      "source": [
        "import folium\n",
        "print(folium.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrXLkJC2QJdP"
      },
      "source": [
        "# Define variables\n",
        "\n",
        "This set of global variables will be used throughout.  For this demo, you must have a Cloud Storage bucket into which you can write files.  ([learn more about creating Cloud Storage buckets](https://cloud.google.com/storage/docs/creating-buckets)).  You'll also need to specify your Earth Engine username, i.e. `users/USER_NAME` on the [Code Editor](https://code.earthengine.google.com/) Assets tab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHTOc5YLQZ5B"
      },
      "source": [
        "# Your Earth Engine username.  This is used to import a classified image\n",
        "# into your Earth Engine assets folder.\n",
        "USER_NAME = 'tanujashrestha'\n",
        "\n",
        "# Cloud Storage bucket into which training, testing and prediction \n",
        "# datasets will be written.  You must be able to write into this bucket.\n",
        "OUTPUT_BUCKET = 'mekong-tf'\n",
        "\n",
        "# Use Landsat 8 surface reflectance data for predictors.\n",
        "L8SR = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "# Use these bands for prediction.\n",
        "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        
        "# The labels, consecutive integer indices starting from zero, are stored in\n",
        "# this property, set on each point.\n",
        "LABEL = 'class'\n",
        "#LABEL = 'landcover'\n",
        "# Number of label values, i.e. number of classes in the classification.\n",
        "#N_CLASSES = 3\n",
        "\n",
        "N_CLASSES = 2\n",
        "\n",
        "# These names are used to specify properties in the export of\n",
        "# training/testing data and to define the mapping between names and data\n",
        "# when reading into TensorFlow datasets.\n",
        "FEATURE_NAMES = list(BANDS)\n",
        "FEATURE_NAMES.append(LABEL)\n",
        "\n",
        "# File names for the training and testing datasets.  These TFRecord files\n",
        "# will be exported from Earth Engine into the Cloud Storage bucket.\n",
        "FOLDER = 'Tanuja_HKH'\n",
        "\n",
        "# File name for the prediction (image) dataset.  The trained model will read\n",
        "# this dataset and make predictions in each pixel.\n",
        "IMAGE_FILE_PREFIX = 'Kathmandu_built_up_Image_pixel_demo_' #make the Kathmandu area\n",
        "\n",
        "# The output path for the classified image (i.e. predictions) TFRecord file.\n",
        "OUTPUT_IMAGE_FILE = 'gs://' + OUTPUT_BUCKET  + '/'  + FOLDER + '/Kathmandu_built_up_Classified_pixel_demo.TFRecord'\n",
        "\n",
        "\n",
        "# Export imagery in this region.\n",
        "\n",
        "EXPORT_REGION = ee.Geometry.Rectangle([85.17477707764515,27.513870899279933, 85.6169767846764,27.807009206815394]) #here is the prediction geometry of Kathmandu\n",
        "\n",
        "# The name of the Earth Engine asset to be created by importing\n",
        "# the classified image from the TFRecord file in Cloud Storage.\n",
        "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/12_02_2021_RLCMS_2D_Ktm_output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcjQnHH8zT4q"
      },
      "source": [
        "# Get Training and Testing data from Earth Engine\n",
        "\n",
        "To get data for a classification model of three classes (bare, vegetation, water), we need labels and the value of predictor variables for each labeled example.  We've already generated some labels in Earth Engine.  Specifically, these are visually interpreted points labeled \"bare,\" \"vegetation,\" or \"water\" for a very simple classification demo ([example script](https://code.earthengine.google.com/?scriptPath=Examples%3ADemos%2FClassification)).  For predictor variables, we'll use [Landsat 8 surface reflectance imagery](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR), bands 2-7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJfjgelSOpN"
      },
      "source": [
        "## Prepare Landsat 8 imagery\n",
        "\n",
        "First, make a cloud-masked median composite of Landsat 8 surface reflectance imagery from 2018.  Check the composite by visualizing with folium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJYucYe3SPPr"
      },
      "source": [
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  return image.updateMask(mask).select(BANDS).divide(10000)\n",
        "\n",
        "# The image input data is a 2018 cloud-masked median composite.\n",
        "image = L8SR.filterDate('2018-01-01', '2018-12-31').map(maskL8sr).median()\n",
        "\n",
        "\n",
        "#LANDSAT/LC08/C01/T1_SR\n",
        "# Use folium to visualize the imagery.\n",
        "# mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "# map = folium.Map(location=[38., -122.5])\n",
        "\n",
        "# folium.TileLayer(\n",
        "#     tiles=mapid['tile_fetcher'].url_format,\n",
        "#     attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "#     overlay=True,\n",
        "#     name='median composite',\n",
        "#   ).add_to(map)\n",
        "# map.add_child(folium.LayerControl())\n",
        "# map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA8QA8oQVo8V"
      },
      "source": [
        "## Export the imagery\n",
        "\n",
        "You can also export imagery using TFRecord format.  Specifically, export whatever imagery you want to be classified by the trained model into the output Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVNhJYacVpEw"
      },
      "source": [
        "# Specify patch and file dimensions.\n",
        "image_export_options = {\n",
        "  'patchDimensions': [256, 256],\n",
        "  'maxFileSize': 104857600,\n",
        "  'compressed': True\n",
        "}\n",
        "\n",
        "# Setup the task.\n",
        "image_task = ee.batch.Export.image.toCloudStorage(\n",
        "  image=image,\n",
        "  description='Image Export',\n",
        "  fileNamePrefix=FOLDER  + '/' + IMAGE_FILE_PREFIX,\n",
        "  bucket=OUTPUT_BUCKET,\n",
        "  scale=30,\n",
        "  fileFormat='TFRecord',\n",
        "  region=EXPORT_REGION.toGeoJSON()['coordinates'],\n",
        "  formatOptions=image_export_options,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SweCkHDaNE3"
      },
      "source": [
        "# Start the task.\n",
        "image_task.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC8C53MRTG_E"
      },
      "source": [
        "### Monitor task progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmPHb779KOXm"
      },
      "source": [
        "# Print all tasks.\n",
        "print(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrUhA1JKLONj"
      },
      "source": [
        "It's also possible to monitor an individual task.  Here we poll the task until it's done.  If you do this, please put a `sleep()` in the loop to avoid making too many requests.  Note that this will block until complete (you can always halt the execution of this cell)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKZeZswloP11"
      },
      "source": [
        "import time\n",
        "\n",
        "while image_task.active():\n",
        "  print('Polling for task (id: {}).'.format(image_task.id))\n",
        "  time.sleep(30)\n",
        "print('Done with image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWdH_wlZCEk"
      },
      "source": [
        "# Data preparation and pre-processing\n",
        "\n",
        "Read data from the TFRecord file into a `tf.data.Dataset`.  Pre-process the dataset to get it into a suitable format for input to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrDYm-ibKR6t"
      },
      "source": [
        "## Define the structure of your data\n",
        "\n",
        "For parsing the exported TFRecord files, `featuresDict` is a mapping between feature names (recall that `featureNames` contains the band and label names) and `float32` [`tf.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) objects.  This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors.  Specifically, **all numeric data exported from Earth Engine is exported as `float32`**.\n",
        "\n",
        "(Note: *features* in the TensorFlow context (i.e. [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature)) are not to be confused with Earth Engine features (i.e. [`ee.Feature`](https://developers.google.com/earth-engine/api_docs#eefeature)), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6JVQV5HKHMZ",
        "cellView": "code"
      },
      "source": [
        "# List of fixed-length features, all of which are float32.\n",
        "columns = [\n",
        "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
        "]\n",
        "\n",
        "# Dictionary with names as keys, features as values.\n",
        "features_dict = dict(zip(FEATURE_NAMES, columns))\n",
        "\n",
        "print(features_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb8EyNT4Xnhb"
      },
      "source": [
        "Note that each record of the parsed dataset contains a tuple.  The first element of the tuple is a dictionary with bands for keys and the numeric value of the bands for values.  The second element of the tuple is a class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTayDitZgQ5"
      },
      "source": [
        "## Find the image files and JSON mixer file in Cloud Storage\n",
        "\n",
        "Use `gsutil` to locate the files of interest in the output Cloud Storage bucket.  Check to make sure your image export task finished before running the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUv9WMpcVp8E"
      },
      "source": [
        "FOLDER = 'Tanuja_HKH'\n",
        "# Get a list of all the files in the output bucket.\n",
        "files_list = !gsutil ls 'gs://'{OUTPUT_BUCKET}'/'{FOLDER}\n",
        "# Get only the files generated by the image export.\n",
        "exported_files_list = [s for s in files_list if IMAGE_FILE_PREFIX in s]\n",
        "\n",
        "# Get the list of image files and the JSON mixer file.\n",
        "image_files_list = []\n",
        "json_file = None\n",
        "for f in exported_files_list:\n",
        "  if f.endswith('.tfrecord.gz'):\n",
        "    image_files_list.append(f)\n",
        "  elif f.endswith('.json'):\n",
        "    json_file = f\n",
        "\n",
        "# Make sure the files are in the right order.\n",
        "image_files_list.sort()\n",
        "\n",
        "print(image_files_list)\n",
        "print(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcjYG9fk53xL"
      },
      "source": [
        "## Read the JSON mixer file\n",
        "\n",
        "The mixer contains metadata and georeferencing information for the exported patches, each of which is in a different file.  Read the mixer to get some information needed for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn7Dr0AAd93_"
      },
      "source": [
        "import json\n",
        "\n",
        "# Load the contents of the mixer file to a JSON object.\n",
        "json_text = !gsutil cat {json_file}\n",
        "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "mixer = json.loads(json_text.nlstr)\n",
        "print(mixer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xyzyPPJwpVI"
      },
      "source": [
        "## Read the image files into a dataset\n",
        "\n",
        "You can feed the list of files (`imageFilesList`) directly to the `TFRecordDataset` constructor to make a combined dataset on which to perform inference.  The input needs to be preprocessed differently than the training and testing.  Mainly, this is because the pixels are written into records as patches, we need to read the patches in as one big tensor (one patch for each band), then flatten them into lots of little tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn8Kj3VfwpiJ",
        "cellView": "code"
      },
      "source": [
        "# Get relevant info from the JSON mixer file.\n",
        "patch_width = mixer['patchDimensions'][0]\n",
        "patch_height = mixer['patchDimensions'][1]\n",
        "patches = mixer['totalPatches']\n",
        "patch_dimensions_flat = [patch_width * patch_height, 1]\n",
        "\n",
        "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
        "image_columns = [\n",
        "  tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) \n",
        "    for k in BANDS\n",
        "]\n",
        "\n",
        "# Parsing dictionary.\n",
        "image_features_dict = dict(zip(BANDS, image_columns))\n",
        "\n",
        "# Note that you can make one dataset from many files by specifying a list.\n",
        "image_dataset = tf.data.TFRecordDataset(image_files_list, compression_type='GZIP')\n",
        "\n",
        "# Parsing function.\n",
        "def parse_image(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, image_features_dict)\n",
        "\n",
        "# Parse the data into tensors, one long tensor per patch.\n",
        "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
        "\n",
        "# Break our long tensors into many little ones.\n",
        "image_dataset = image_dataset.flat_map(\n",
        "  lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
        ")\n",
        "\n",
        "# # Add additional features (NDVI).\n",
        "# image_dataset = image_dataset.map(\n",
        "#   # Add NDVI to a feature that doesn't have a label.\n",
        "#   lambda features: add_NDVI(features, None)[0]\n",
        "# )\n",
        "\n",
        "# Turn the dictionary in each record into a tuple without a label.\n",
        "image_dataset = image_dataset.map(\n",
        "  lambda data_dict: (tf.transpose(list(data_dict.values())), )\n",
        ")\n",
        "\n",
        "# Turn each patch into a batch.\n",
        "image_dataset = image_dataset.batch(patch_width * patch_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2sfRemRRDkV"
      },
      "source": [
        "## Generate predictions for the image pixels\n",
        "\n",
        "To get predictions in each pixel, run the image dataset through the trained model using `model.predict()`.  Print the first prediction to see that the output is a list of the three class probabilities for each pixel.  Running all predictions might take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ1SwSJGof4V"
      },
      "source": [
        "#Loading model from google drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/mnt')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEsCw1ms5DQG"
      },
      "source": [
        "%cd /content/mnt/My Drive\r\n",
        "%cd cambodia\r\n",
        "%ls\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IHAbKN0DsSf"
      },
      "source": [
        "#to save the model - here we are using already saved model\r\n",
        "!mkdir -p saved_model\r\n",
        "model.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waXkM3Zj5Dy1"
      },
      "source": [
        "new_model = tf.keras.models.load_model('saved_model/my_model')\r\n",
        "new_model = tf.keras.models.load_model('saved_model_new_polygons/my_model')\r\n",
        "new_model = tf.keras.models.load_model('saved_model_new_polygons_02_02_2021/my_model')\r\n",
        "# Check its architecture\r\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VGhmiP_REBP"
      },
      "source": [
        "# Run prediction in batches, with as many steps as there are patches.\n",
        "predictions = model.predict(image_dataset, steps=patches, verbose=1)\n",
        "\n",
        "predictions = new_model.predict(image_dataset, steps=patches, verbose=1)\n",
        "\n",
        "# Note that the predictions come as a numpy array.  Check the first one.\n",
        "print(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPU2VlPOikAy"
      },
      "source": [
        "## Write the predictions to a TFRecord file\n",
        "\n",
        "Now that there's a list of class probabilities in `predictions`, it's time to write them back into a file, optionally including a class label which is simply the index of the maximum probability.  We'll write directly from TensorFlow to a file in the output Cloud Storage bucket.\n",
        "\n",
        "Iterate over the list, compute class label and write the class and the probabilities in patches.  Specifically, we need to write the pixels into the file as patches in the same order they came out.  The records are written as serialized `tf.train.Example` protos.  This might take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkorbsEHepzJ"
      },
      "source": [
        "print('Writing to file ' + OUTPUT_IMAGE_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kATMknHc0qeR",
        "cellView": "code"
      },
      "source": [
        "# Instantiate the writer.\n",
        "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n",
        "\n",
        "# Every patch-worth of predictions we'll dump an example into the output\n",
        "# file with a single feature that holds our predictions. Since our predictions\n",
        "# are already in the order of the exported data, the patches we create here\n",
        "# will also be in the right order.\n",
        "patch = [[], [], [], []]\n",
        "cur_patch = 1\n",
        "for prediction in predictions:\n",
        "  patch[0].append(tf.argmax(prediction, 1))\n",
        "  patch[1].append(prediction[0][0])\n",
        "  patch[2].append(prediction[0][1])\n",
        "  #patch[3].append(prediction[0][2])\n",
        "  # Once we've seen a patches-worth of class_ids...\n",
        "  if (len(patch[0]) == patch_width * patch_height):\n",
        "    print('Done with patch ' + str(cur_patch) + ' of ' + str(patches) + '...')\n",
        "    # Create an example\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'prediction': tf.train.Feature(\n",
        "              int64_list=tf.train.Int64List(\n",
        "                  value=patch[0])),\n",
        "          'built': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=patch[1])),\n",
        "          'non_built': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=patch[2]))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example to the file and clear our patch array so it's ready for\n",
        "    # another batch of class ids\n",
        "    writer.write(example.SerializeToString())\n",
        "    patch = [[], [], [], []]\n",
        "    cur_patch += 1\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K_1hKs0aBdA"
      },
      "source": [
        "# Upload the classifications to an Earth Engine asset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6sNZXWOSa82"
      },
      "source": [
        "## Verify the existence of the predictions file\n",
        "\n",
        "At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket.  Use the `gsutil` command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZVWDPefUCgA"
      },
      "source": [
        "!gsutil ls -l {OUTPUT_IMAGE_FILE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZyCo297Clcx"
      },
      "source": [
        "## Upload the classified image to Earth Engine\n",
        "\n",
        "Upload the image to Earth Engine directly from the Cloud Storage bucket with the [`earthengine` command](https://developers.google.com/earth-engine/command_line#upload).  Provide both the image TFRecord file and the JSON file as arguments to `earthengine upload`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXulMNl9lTDv",
        "cellView": "code"
      },
      "source": [
        "print('Uploading to ' + OUTPUT_ASSET_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64tcVxsO5h6"
      },
      "source": [
        "# Start the upload.\n",
        "!earthengine upload image --asset_id={OUTPUT_ASSET_ID} --pyramiding_policy=mode {OUTPUT_IMAGE_FILE} {json_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt4HyhUU_Bal"
      },
      "source": [
        "## Check the status of the asset ingestion\n",
        "\n",
        "You can also use the Earth Engine API to check the status of your asset upload.  It might take a while.  The upload of the image is an asset ingestion task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vB-gwGhl_3C",
        "cellView": "code"
      },
      "source": [
        "ee.batch.Task.list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvXvy9GDhM-p"
      },
      "source": [
        "## View the ingested asset\n",
        "\n",
        "Display the vector of class probabilities as an RGB image with colors corresponding to the probability of bare, vegetation, water in a pixel.  Also display the winning class using the same color palette."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEkVxIyJiFd4"
      },
      "source": [
        "#predictions_image = ee.Image('users/tanujashrestha/built_up_Classified_pixel_demo')\n",
        "predictions_image = ee.Image('users/tanujashrestha/01_02_2021_New_Kathmandu_built_up_Classified_pixel_demo')\n",
        "\n",
        "prediction_vis = {\n",
        "  'bands': 'prediction',\n",
        "  'min': 0.5,\n",
        "  'max': 1,\n",
        "  'palette': ['black', 'blue']\n",
        "}\n",
        "probability_vis = {'bands': ['built', 'non_built'], 'max': 0.5}\n",
        "\n",
        "prediction_map_id = predictions_image.getMapId(prediction_vis)\n",
        "probability_map_id = predictions_image.getMapId(probability_vis)\n",
        "\n",
        "#map = folium.Map(location=[37.6413, -122.2582])\n",
        "map = folium.Map(location=[28.39, 84.12])\n",
        "folium.TileLayer(\n",
        "  tiles=prediction_map_id['tile_fetcher'].url_format,\n",
        "  attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "  overlay=True,\n",
        "  name='prediction',\n",
        ").add_to(map)\n",
        "\n",
        "# folium.TileLayer(\n",
        "#   tiles=probability_map_id['tile_fetcher'].url_format,\n",
        "#   attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "#   overlay=True,\n",
        "#   name='probability',\n",
        "# ).add_to(map)\n",
        "# map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
